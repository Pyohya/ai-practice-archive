# 문자 단위 순환 신경망(LSTM) 텍스트 생성 예제

이 디렉토리에는 문자 단위(character-level)로 텍스트를 생성하는 간단한 순환 신경망 (LSTM) 모델을 Python과 TensorFlow/Keras로 구현한 예제 (`generate_text.py`)가 포함되어 있습니다.

## 1. 문자 단위 RNN/LSTM 텍스트 생성 소개

**텍스트 생성**은 모델이 주어진 텍스트 데이터의 패턴을 학습하여 새로운 텍스트를 만들어내는 기술입니다.

**문자 단위 생성 방식**은 단어나 형태소 대신 개별 문자를 기본 단위로 사용합니다.
*   **장점**: 어휘사전(vocabulary)의 크기가 작고, 신조어나 오타 등 모든 단어를 처리할 수 있으며, 구현이 비교적 간단합니다.
*   **단점**: 단어 단위보다 더 긴 시퀀스를 학습해야 문맥을 파악할 수 있습니다.

**RNN (Recurrent Neural Network) / LSTM (Long Short-Term Memory)** 은 시퀀스 데이터 처리에 특화된 신경망입니다. 이전 단계의 정보를 기억(memory)하여 다음 단계의 예측에 활용합니다. LSTM은 특히 긴 시퀀스에서 RNN의 장기 의존성 문제를 개선한 모델입니다.

문자 단위 텍스트 생성에서 LSTM 모델은 이전 문자들의 시퀀스를 바탕으로 **다음에 올 가장 확률이 높은 문자**를 예측하는 방식으로 학습합니다: `P(다음 문자 | 이전 문자들)`.

## 2. `generate_text.py` 코드 상세 설명

### 2.1. 주요 의존성 (Dependencies)

스크립트 실행에는 다음 라이브러리가 필요합니다:
*   **TensorFlow**: 딥러닝 모델 구성 및 학습 (`tf.keras` 사용).
*   **NumPy**: 수치 계산 및 데이터 배열 처리.

다음 명령어로 설치할 수 있습니다:
```bash
pip install tensorflow numpy
```

### 2.2. 주요 파라미터

스크립트 상단에서 주요 파라미터를 설정합니다:
*   `SEQ_LENGTH`: 모델이 한 번에 입력으로 받는 문자의 시퀀스 길이 (예: 50).
*   `EPOCHS`: 전체 학습 데이터셋을 반복 학습하는 횟수 (예: 50).
*   `BATCH_SIZE`: 한 번의 학습 단계에서 사용되는 데이터 샘플의 수 (예: 128).
*   `LSTM_UNITS`: LSTM 계층의 내부 유닛(메모리 셀) 수 (예: 128).
*   `EMBEDDING_DIM`: 문자 임베딩 벡터의 차원 수 (예: 64).

### 2.3. 데이터 로딩 (`load_corpus` 함수)

*   `scripts/text_generation_example/corpus.txt` 파일에서 학습용 텍스트를 읽어옵니다.
*   간단한 전처리로 불필요한 연속 줄바꿈 문자를 제거하고 앞뒤 공백을 제거합니다.
*   파일이 없을 경우, 간단한 더미 텍스트를 사용하도록 예외 처리가 되어 있습니다.

### 2.4. 텍스트 전처리 (`preprocess_text` 함수)

*   로딩된 전체 텍스트에서 중복을 제외한 모든 고유 문자를 찾아 정렬하여 **어휘집(vocabulary)**을 만듭니다.
*   각 문자에 고유한 정수 인덱스를 부여하는 `char_to_int` 딕셔너리와, 그 반대인 `int_to_char` 딕셔너리를 생성합니다. 이는 신경망이 문자를 처리할 수 있도록 변환하는 데 필요합니다.

### 2.5. 학습 데이터 준비 (`prepare_training_data` 함수)

*   전체 텍스트를 `SEQ_LENGTH` 길이의 입력 시퀀스 (`X`)와 바로 다음에 오는 문자 (`y`) 쌍으로 분할합니다.
    *   예: `SEQ_LENGTH`가 5이고 텍스트가 "안녕하세요"라면,
        *   `X` = "안녕하세", `y` = "요"
*   생성된 `X`와 `y`의 각 문자를 `char_to_int`를 사용해 정수 인덱스로 변환합니다.
*   `X`는 `(총 샘플 수, SEQ_LENGTH)` 형태의 2D 배열로, `y`는 `(총 샘플 수,)` 형태의 1D 배열로 준비됩니다.

### 2.6. LSTM 모델 정의 (`build_model` 함수)

`tf.keras.Sequential` 모델을 사용하여 다음 세 개의 주요 계층으로 구성됩니다:

1.  **`Embedding` Layer**:
    *   **역할**: 정수로 인코딩된 각 문자 인덱스를 `EMBEDDING_DIM` 차원의 밀집 벡터(dense vector)로 변환합니다. 이 벡터는 문자의 의미론적 특징을 학습하려고 시도합니다 (문맥상 비슷한 위치에 나오는 문자들은 유사한 벡터값을 가질 수 있음).
    *   **입력**: `(배치 크기, SEQ_LENGTH)` 형태의 정수 시퀀스.
    *   **출력**: `(배치 크기, SEQ_LENGTH, EMBEDDING_DIM)` 형태의 실수형 텐서.

2.  **`LSTM` Layer**:
    *   **역할**: `Embedding` 계층에서 변환된 시퀀스 벡터를 입력받아, 시간적(순서적) 패턴을 학습합니다. LSTM의 내부 셀 구조는 장기 기억을 가능하게 하여 시퀀스 내의 중요한 정보를 유지하고 다음 문자를 예측하는 데 사용합니다.
    *   **입력**: `(배치 크기, SEQ_LENGTH, EMBEDDING_DIM)` 형태의 텐서.
    *   **출력**: (기본적으로 마지막 타임스텝의 출력이지만, 여기서는 다음 Dense 계층으로 전체 시퀀스에 대한 최종 상태가 전달됨) `(배치 크기, LSTM_UNITS)` 형태의 텐서.

3.  **`Dense` Layer (출력 계층)**:
    *   **역할**: LSTM 계층의 출력을 받아 어휘집(vocabulary) 크기의 벡터로 변환하고, `softmax` 활성화 함수를 통해 각 문자가 다음에 올 확률 분포를 계산합니다.
    *   **입력**: `(배치 크기, LSTM_UNITS)` 형태의 텐서.
    *   **출력**: `(배치 크기, 어휘집 크기)` 형태의 확률 분포 텐서.

### 2.7. 모델 학습 (메인 실행 블록 `if __name__ == '__main__':` 내부)

*   **컴파일**: `model.compile()`
    *   `optimizer='adam'`: Adam 최적화 알고리즘 사용.
    *   `loss='sparse_categorical_crossentropy'`: `y` 타겟이 원-핫 인코딩이 아닌 정수 인덱스 형태일 때 사용하는 손실 함수.
    *   `metrics=['accuracy']`: 학습 중 정확도를 모니터링.
*   **학습 실행**: `model.fit()`
    *   준비된 `X_train` (입력 시퀀스)과 `y_train` (타겟 문자)을 사용하여 모델을 지정된 `EPOCHS` 및 `BATCH_SIZE` 만큼 학습시킵니다.

### 2.8. 텍스트 생성 함수 (`generate_text` 함수)

*   **입력**: 학습된 모델, 문자 매핑 딕셔너리, 초기 **시드(seed) 텍스트**, 생성할 문자의 수.
*   **초기화**: 시드 텍스트를 정수 시퀀스로 변환합니다.
*   **반복적 예측 및 샘플링**:
    1.  현재까지 생성된 시퀀스 (초기에는 시드 텍스트)를 모델의 입력 형식에 맞게 준비합니다. (필요시 앞부분을 0으로 채우거나(`padding`) `SEQ_LENGTH`에 맞게 자릅니다.)
    2.  모델(`model.predict()`)은 다음 문자에 대한 확률 분포를 예측합니다.
    3.  `tf.random.categorical` 함수를 사용하여 예측된 확률 분포에 따라 다음 문자를 **샘플링**합니다. (가장 확률 높은 문자만 선택하는 대신 샘플링을 통해 좀 더 다양하고 자연스러운 텍스트 생성을 유도합니다.)
    4.  샘플링된 문자를 결과 텍스트에 추가하고, 다음 반복을 위해 현재 시퀀스를 업데이트합니다.
    5.  원하는 길이만큼 문자가 생성될 때까지 반복합니다.

## 3. 사용 방법

### 3.1. 학습 데이터 준비 (`corpus.txt`)

*   스크립트는 실행되는 디렉토리 내의 `scripts/text_generation_example/corpus.txt` 파일을 학습 데이터로 사용합니다.
*   **사용자 정의 데이터 사용**:
    1.  원하는 한국어 텍스트 파일을 준비합니다.
    2.  파일명을 `corpus.txt`로 변경하거나, `load_corpus` 함수 내의 파일 경로를 수정합니다.
    3.  파일 인코딩은 UTF-8이어야 합니다.
*   **데이터 품질**:
    *   데이터의 양이 많고 다양할수록 생성되는 텍스트의 품질이 향상될 수 있습니다. (하지만 현재 모델은 간단하여 한계가 있습니다.)
    *   매우 작은 데이터셋은 모델이 단순히 해당 텍스트를 암기하게 만들어(과적합), 새롭고 의미 있는 텍스트 생성이 어려울 수 있습니다.

### 3.2. 스크립트 실행 (`generate_text.py`)

1.  터미널 또는 명령 프롬프트를 엽니다.
2.  `scripts/text_generation_example` 디렉토리로 이동합니다.
    ```bash
    cd scripts/text_generation_example
    ```
3.  다음 명령어로 파이썬 스크립트를 실행합니다:
    ```bash
    python generate_text.py
    ```
    (만약 저장소 최상위 디렉토리에서 실행한다면 `python scripts/text_generation_example/generate_text.py` 와 같이 전체 경로를 사용합니다.)

### 3.3. 예상 출력

스크립트를 실행하면 다음과 같은 정보가 콘솔에 차례로 출력됩니다:

1.  **데이터 로딩 정보**: 불러온 `corpus.txt`의 총 문자 수, 고유 문자 수 및 목록.
2.  **학습 패턴 수**: 생성된 (입력 시퀀스, 타겟 문자) 쌍의 총 개수.
3.  **모델 요약**: `model.summary()`를 통해 본 신경망의 각 계층 구조와 파라미터 수.
4.  **학습 진행 상황**: 각 에포크(epoch)마다 손실(loss) 값과 정확도(accuracy)가 출력됩니다.
    ```
    Epoch 1/50
    5/5 - 4s - 707ms/step - accuracy: 0.1545 - loss: 4.8335
    ...
    Epoch 50/50
    5/5 - 1s - 213ms/step - accuracy: 0.5764 - loss: 1.8781
    ```
5.  **생성된 텍스트**: 학습 완료 후, 스크립트에 정의된 시드 텍스트(들)를 기반으로 생성된 새로운 텍스트가 출력됩니다.
    ```
    --- Generating text with seed: '...' ---
    --- Generated Text (from corpus seed) ---
    ... (생성된 텍스트) ...

    --- Generating text with seed: '호랑이는 엄마에게' ---
    --- Generated Text (from custom seed) ---
    ... (생성된 텍스트) ...
    ```
**주의**: 제공된 `corpus.txt`는 매우 작고, `EPOCHS`도 상대적으로 적기 때문에 생성되는 텍스트는 문법적으로 완벽하거나 의미가 깊지는 않을 것입니다. 기본적인 문자열 패턴(예: 자주 나오는 글자 조합, 간단한 단어의 일부)을 학습하는 수준을 보여줍니다. 더 나은 품질의 텍스트를 원한다면 훨씬 큰 데이터셋과 더 많은 학습 시간이 필요합니다.

## 4. 필요 라이브러리 재확인

스크립트 실행에 필요한 주요 라이브러리는 다음과 같습니다:
*   TensorFlow (`tensorflow`)
*   NumPy (`numpy`)

설치 명령어:
```bash
pip install tensorflow numpy
```
