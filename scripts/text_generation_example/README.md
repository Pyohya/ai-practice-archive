# 문자 단위 순환 신경망(LSTM) 텍스트 생성 예제

이 디렉토리에는 문자 단위(character-level)로 텍스트를 생성하는 간단한 순환 신경망 (LSTM) 모델을 Python과 TensorFlow/Keras로 구현한 예제 (`generate_text.py`)가 포함되어 있습니다.

## 1. 문자 단위 RNN/LSTM 텍스트 생성 소개

**텍스트 생성**은 모델이 주어진 텍스트 데이터의 패턴을 학습하여 새로운 텍스트를 만들어내는 기술입니다.

**문자 단위 생성 방식**은 단어나 형태소 대신 개별 문자를 기본 단위로 사용합니다.
*   **장점**: 어휘사전(vocabulary)의 크기가 작고, 신조어나 오타 등 모든 단어를 처리할 수 있으며, 구현이 비교적 간단합니다.
*   **단점**: 단어 단위보다 더 긴 시퀀스를 학습해야 문맥을 파악할 수 있습니다.

**RNN (Recurrent Neural Network) / LSTM (Long Short-Term Memory)** 은 시퀀스 데이터 처리에 특화된 신경망입니다. 이전 단계의 정보를 기억(memory)하여 다음 단계의 예측에 활용합니다. LSTM은 특히 긴 시퀀스에서 RNN의 장기 의존성 문제를 개선한 모델입니다.

문자 단위 텍스트 생성에서 LSTM 모델은 이전 문자들의 시퀀스를 바탕으로 **다음에 올 가장 확률이 높은 문자**를 예측하는 방식으로 학습합니다: `P(다음 문자 | 이전 문자들)`.

## 2. `generate_text.py` 코드 상세 설명

### 2.1. 주요 의존성 (Dependencies)

스크립트 실행에는 다음 라이브러리가 필요합니다:
*   **TensorFlow**: 딥러닝 모델 구성 및 학습 (`tf.keras` 사용).
*   **NumPy**: 수치 계산 및 데이터 배열 처리.

다음 명령어로 설치할 수 있습니다:
```bash
pip install tensorflow numpy
```

### 2.2. 주요 파라미터

스크립트 상단에서 주요 파라미터를 설정합니다:
*   `SEQ_LENGTH`: 모델이 한 번에 입력으로 받는 문자의 시퀀스 길이 (예: 50).
*   `EPOCHS`: 전체 학습 데이터셋을 반복 학습하는 횟수 (현재 150으로 설정됨). 더 많은 학습을 통해 패턴을 더 잘 학습할 수 있지만, 학습 시간이 크게 증가합니다.
*   `BATCH_SIZE`: 한 번의 학습 단계에서 사용되는 데이터 샘플의 수 (예: 128).
*   `LSTM_UNITS`: (스크립트 내 `build_model` 함수에서는 현재 256으로 고정됨) LSTM 계층의 내부 유닛(메모리 셀) 수.
*   `EMBEDDING_DIM`: 문자 임베딩 벡터의 차원 수 (예: 64).

### 2.3. 데이터 로딩 (`load_corpus` 함수)

*   `scripts/text_generation_example/corpus.txt` 파일에서 학습용 텍스트를 읽어옵니다.
*   간단한 전처리로 불필요한 연속 줄바꿈 문자를 제거하고 앞뒤 공백을 제거합니다.
*   파일이 없을 경우, 간단한 더미 텍스트를 사용하도록 예외 처리가 되어 있습니다.

### 2.4. 텍스트 전처리 (`preprocess_text` 함수)

*   로딩된 전체 텍스트에서 중복을 제외한 모든 고유 문자를 찾아 정렬하여 **어휘집(vocabulary)**을 만듭니다.
*   각 문자에 고유한 정수 인덱스를 부여하는 `char_to_int` 딕셔너리와, 그 반대인 `int_to_char` 딕셔너리를 생성합니다. 이는 신경망이 문자를 처리할 수 있도록 변환하는 데 필요합니다.

### 2.5. 학습 데이터 준비 (`prepare_training_data` 함수)

*   전체 텍스트를 `SEQ_LENGTH` 길이의 입력 시퀀스 (`X`)와 바로 다음에 오는 문자 (`y`) 쌍으로 분할합니다.
    *   예: `SEQ_LENGTH`가 5이고 텍스트가 "안녕하세요"라면,
        *   `X` = "안녕하세", `y` = "요"
*   생성된 `X`와 `y`의 각 문자를 `char_to_int`를 사용해 정수 인덱스로 변환합니다.
*   `X`는 `(총 샘플 수, SEQ_LENGTH)` 형태의 2D 배열로, `y`는 `(총 샘플 수,)` 형태의 1D 배열로 준비됩니다.

### 2.6. LSTM 모델 정의 (`build_model` 함수)

`tf.keras.Sequential` 모델을 사용하며, 수정된 아키텍처는 다음과 같습니다 (더 깊은 학습을 위해 적층 LSTM 구조 사용):

1.  **`Embedding` Layer**:
    *   **역할**: 이전과 동일하게, 정수 인코딩된 문자 인덱스를 `EMBEDDING_DIM` 차원의 밀집 벡터로 변환합니다.
    *   **입력**: `(배치 크기, SEQ_LENGTH)` 형태의 정수 시퀀스.
    *   **출력**: `(배치 크기, SEQ_LENGTH, EMBEDDING_DIM)` 형태의 실수형 텐서.

2.  **첫 번째 `LSTM` Layer**:
    *   **유닛 수**: 256.
    *   **`return_sequences=True`**: 이 옵션이 True이면 각 타임스텝(문자)마다 출력을 생성하여 다음 LSTM 계층으로 전달합니다. (적층 LSTM을 위해 필요).
    *   **역할**: 임베딩된 시퀀스를 입력받아 첫 번째 수준의 시간적 패턴을 학습합니다.

3.  **`Dropout` Layer**:
    *   **비율**: 0.2 (20%의 유닛을 임의로 비활성화).
    *   **역할**: 첫 번째 LSTM 계층의 출력에 드롭아웃을 적용하여 과적합(overfitting)을 방지하는 데 도움을 줍니다.

4.  **두 번째 `LSTM` Layer**:
    *   **유닛 수**: 256.
    *   **`return_sequences=False`** (기본값): 이 LSTM 계층은 시퀀스의 마지막 타임스텝에서만 출력을 생성합니다.
    *   **역할**: 이전 LSTM 계층의 출력 시퀀스를 받아 더 복잡한 패턴을 학습합니다.

5.  **`Dropout` Layer**:
    *   **비율**: 0.2.
    *   **역할**: 두 번째 LSTM 계층의 출력에 드롭아웃을 적용하여 추가적인 과적합 방지를 돕습니다.

6.  **`Dense` Layer (출력 계층)**:
    *   **역할**: 최종 LSTM 계층의 출력을 받아 어휘집 크기의 벡터로 변환하고, `softmax` 활성화 함수를 통해 각 문자가 다음에 올 확률 분포를 계산합니다.
    *   **입력**: `(배치 크기, 256)` 형태의 텐서 (두 번째 LSTM 유닛 수와 동일).
    *   **출력**: `(배치 크기, 어휘집 크기)` 형태의 확률 분포 텐서.

### 2.7. 모델 학습 (메인 실행 블록 `if __name__ == '__main__':` 내부)

*   **컴파일**: `model.compile()`
    *   `optimizer='adam'`: Adam 최적화 알고리즘 사용.
    *   `loss='sparse_categorical_crossentropy'`: `y` 타겟이 원-핫 인코딩이 아닌 정수 인덱스 형태일 때 사용하는 손실 함수.
    *   `metrics=['accuracy']`: 학습 중 정확도를 모니터링.
*   **학습 실행**: `model.fit()`
    *   준비된 `X_train` (입력 시퀀스)과 `y_train` (타겟 문자)을 사용하여 모델을 지정된 `EPOCHS` 및 `BATCH_SIZE` 만큼 학습시킵니다.

### 2.8. 텍스트 생성 함수 (`generate_text` 함수)

*   **입력**: 학습된 모델, 문자 매핑 딕셔너리, 초기 **시드(seed) 텍스트**, 생성할 문자의 수.
*   **초기화**: 시드 텍스트를 정수 시퀀스로 변환합니다.
*   **반복적 예측 및 샘플링**:
    1.  현재까지 생성된 시퀀스 (초기에는 시드 텍스트)를 모델의 입력 형식에 맞게 준비합니다. (필요시 앞부분을 0으로 채우거나(`padding`) `SEQ_LENGTH`에 맞게 자릅니다.)
    2.  모델(`model.predict()`)은 다음 문자에 대한 확률 분포를 예측합니다.
    3.  `tf.random.categorical` 함수를 사용하여 예측된 확률 분포에 따라 다음 문자를 **샘플링**합니다. (가장 확률 높은 문자만 선택하는 대신 샘플링을 통해 좀 더 다양하고 자연스러운 텍스트 생성을 유도합니다.)
    4.  샘플링된 문자를 결과 텍스트에 추가하고, 다음 반복을 위해 현재 시퀀스를 업데이트합니다.
    5.  원하는 길이만큼 문자가 생성될 때까지 반복합니다.

## 3. 사용 방법

### 3.1. 학습 데이터 준비 (`corpus.txt`)

*   스크립트는 실행되는 디렉토리 내의 `scripts/text_generation_example/corpus.txt` 파일을 학습 데이터로 사용합니다.
*   **사용자 정의 데이터 사용**:
    1.  원하는 한국어 텍스트 파일을 준비합니다.
    2.  파일명을 `corpus.txt`로 변경하거나, `load_corpus` 함수 내의 파일 경로를 수정합니다.
    3.  파일 인코딩은 UTF-8이어야 합니다.
*   **데이터 품질**:
    *   데이터의 양이 많고 다양할수록 생성되는 텍스트의 품질이 향상될 수 있습니다. (하지만 현재 모델은 간단하여 한계가 있습니다.)
    *   매우 작은 데이터셋은 모델이 단순히 해당 텍스트를 암기하게 만들어(과적합), 새롭고 의미 있는 텍스트 생성이 어려울 수 있습니다.

### 3.2. 스크립트 실행 (`generate_text.py`)

1.  터미널 또는 명령 프롬프트를 엽니다.
2.  `scripts/text_generation_example` 디렉토리로 이동합니다.
    ```bash
    cd scripts/text_generation_example
    ```
3.  다음 명령어로 파이썬 스크립트를 실행합니다:
    ```bash
    python generate_text.py
    ```
    (만약 저장소 최상위 디렉토리에서 실행한다면 `python scripts/text_generation_example/generate_text.py` 와 같이 전체 경로를 사용합니다.)

### 3.3. 예상 출력

스크립트를 실행하면 다음과 같은 정보가 콘솔에 차례로 출력됩니다:

1.  **데이터 로딩 정보**: 불러온 `corpus.txt`의 총 문자 수, 고유 문자 수 및 목록.
2.  **학습 패턴 수**: 생성된 (입력 시퀀스, 타겟 문자) 쌍의 총 개수.
3.  **모델 요약**: `model.summary()`를 통해 본 신경망의 각 계층 구조와 파라미터 수.
4.  **학습 진행 상황**: 각 에포크(epoch)마다 손실(loss) 값과 정확도(accuracy)가 출력됩니다.
    ```
    Epoch 1/150
    ...
    Epoch 150/150
    ... - accuracy: 1.0000 - loss: 0.0230 
    ```
    (정확한 손실/정확도 값은 실행 시마다 다를 수 있습니다.)

5.  **생성된 텍스트**: 학습 완료 후, 스크립트에 정의된 시드 텍스트(들)를 기반으로 생성된 새로운 텍스트가 출력됩니다.
    ```
    --- Generating text with seed: '...' ---
    --- Generated Text (from corpus seed) ---
    ... (생성된 텍스트) ...

    --- Generating text with seed: '호랑이는 엄마에게' ---
    --- Generated Text (from custom seed) ---
    ... (생성된 텍스트) ...
    ```
**주의 및 고려사항**:
*   **학습 시간**: `EPOCHS`가 150으로 증가함에 따라 학습 시간이 이전보다 상당히 길어집니다.
*   **과적합(Overfitting)**: 제공된 `corpus.txt`는 매우 작습니다. 이렇게 작은 데이터셋으로 많은 에포크(예: 150)를 학습하면 모델이 학습 데이터를 거의 암기하게 될 가능성이 높습니다 (과적합). 이 경우, 생성된 텍스트는 학습 데이터의 특정 부분을 그대로 반복하거나 매우 유사하게 보일 수 있으며, 새롭거나 창의적인 텍스트를 생성하는 능력은 떨어질 수 있습니다.
*   **텍스트 품질**: 현재 설정(작은 코퍼스, 적층 LSTM)에서 생성되는 텍스트는 기본적인 문자열 패턴과 짧은 단어 조합을 학습할 수 있지만, 문법적 완벽성이나 장기적인 문맥적 일관성을 기대하기는 어렵습니다. 의미 있고 자연스러운 텍스트 생성을 위해서는 훨씬 더 큰 고품질의 데이터셋, 정교한 모델 아키텍처, 그리고 세심한 하이퍼파라미터 튜닝 및 긴 학습 시간이 필요합니다.

## 4. 필요 라이브러리 재확인

스크립트 실행에 필요한 주요 라이브러리는 다음과 같습니다:
*   TensorFlow (`tensorflow`)
*   NumPy (`numpy`)

설치 명령어:
```bash
pip install tensorflow numpy
```
